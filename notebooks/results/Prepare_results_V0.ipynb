{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97eb431e-320e-47e0-8624-3c84b14dfe85",
   "metadata": {},
   "source": [
    "# Prepare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5ac204-fc9f-457e-b28d-3e8e0ce0cbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT /scratch/cs/world-models/merlerm1/open-world-symbolic-planner\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datasets\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "ROOT = os.path.dirname(os.path.dirname(os.path.abspath('.')))\n",
    "print(\"ROOT\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72cdcc51-54a6-4192-9ae6-e92cdeeeff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_winoground_main_results(results):\n",
    "    scores = {\n",
    "        'overall': 0,\n",
    "        'strict': 0,\n",
    "        'positive': 0,\n",
    "        'negative': 0,\n",
    "    }\n",
    "    \n",
    "    for id, result in results['results'].items():\n",
    "        tmp_count = 0\n",
    "        for answer_key, answer in result.items():\n",
    "            # print(answer)\n",
    "            if answer['answer'][0].lower().strip() == answer['gt_answer'].lower().strip():\n",
    "                scores['overall'] += 1\n",
    "                tmp_count += 1\n",
    "                if answer['gt_answer'].lower().strip() == 'yes':\n",
    "                    scores['positive'] += 1\n",
    "                elif answer['gt_answer'].lower().strip() == 'no':\n",
    "                    scores['negative'] += 1\n",
    "        if tmp_count == len(result):\n",
    "            scores['strict'] += 1\n",
    "        \n",
    "    scores['overall_frac'] = scores['overall'] / (len(results['results']) * len(results['results']['0']))\n",
    "    scores['strict_frac'] = scores['strict'] / len(results['results'])\n",
    "    scores['positive_frac'] = scores['positive'] / (len(results['results']) * (len(results['results']['0']) / 2))\n",
    "    scores['negative_frac'] = scores['negative'] / (len(results['results']) * (len(results['results']['0']) / 2))\n",
    "\n",
    "    yes_count = 0\n",
    "    for id, result in results['results'].items():\n",
    "        for answer_key, answer in result.items():\n",
    "            if answer['answer'][0].lower().strip() == 'yes':\n",
    "                yes_count += 1\n",
    "                \n",
    "    scores['yes_frac'] = yes_count / (len(results['results']) * (len(results['results']['0'])))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "813795dc-d085-47a9-a1a8-1dce5f51afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coco_main_results(results, dataset):\n",
    "    scores = {\n",
    "        \"adversarial\": 0,\n",
    "        \"popular\": 0,\n",
    "        \"random\": 0\n",
    "    }\n",
    "\n",
    "    for item in dataset['validation']:\n",
    "        img_id = str(item['id'])\n",
    "        if img_id in results['adversarial_q'].keys():\n",
    "            for i, answer in enumerate(results['adversarial_q'][img_id]):\n",
    "                if answer.strip().lower() == item['adversarial_a'][i].strip().lower():\n",
    "                    scores[\"adversarial\"] += 1\n",
    "                    \n",
    "        if img_id in results['popular_q'].keys():\n",
    "            for i, answer in enumerate(results['popular_q'][img_id]):\n",
    "                if answer.strip().lower() == item['popular_a'][i].strip().lower():\n",
    "                    scores[\"popular\"] += 1\n",
    "                    \n",
    "        if img_id in results['random_q'].keys():\n",
    "            for i, answer in enumerate(results['random_q'][img_id]):\n",
    "                if answer.strip().lower() == item['random_a'][i].strip().lower():\n",
    "                    scores[\"random\"] += 1\n",
    "                        \n",
    "    QUESTIONS_PER_ITEM = len(dataset['validation'][0]['adversarial_q'])\n",
    "\n",
    "    main_result = {\n",
    "        \"adversarial\": np.round(scores[\"adversarial\"] / (len(dataset['validation']) * QUESTIONS_PER_ITEM), 4),\n",
    "        \"popular\": np.round(scores[\"popular\"] / (len(dataset['validation']) * QUESTIONS_PER_ITEM), 4),\n",
    "        \"random\": np.round(scores[\"random\"] / (len(dataset['validation']) * QUESTIONS_PER_ITEM), 4)\n",
    "    }\n",
    "    print(\"Adversarial: \", main_result[\"adversarial\"])\n",
    "    print(\"Popular: \", main_result[\"popular\"])\n",
    "    print(\"Random: \", main_result[\"random\"])\n",
    "    return main_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6691c0f2-1700-4276-9f9a-3a330ad2d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_oi_main_results(results, dataset):\n",
    "    scores = {\n",
    "        \"adversarial\": 0,\n",
    "        \"popular\": 0,\n",
    "        \"random\": 0\n",
    "    }\n",
    "\n",
    "    for item in dataset['validation']:\n",
    "        img_id = str(item['id'])\n",
    "        if img_id in results['adversarial_q'].keys():\n",
    "            for i, answer in enumerate(results['adversarial_q'][img_id]):\n",
    "                if answer.strip().lower() == item['adversarial_a'][i].strip().lower():\n",
    "                    scores[\"adversarial\"] += 1\n",
    "                    \n",
    "        if img_id in results['popular_q'].keys():\n",
    "            for i, answer in enumerate(results['popular_q'][img_id]):\n",
    "                if answer.strip().lower() == item['popular_a'][i].strip().lower():\n",
    "                    scores[\"popular\"] += 1\n",
    "                    \n",
    "        if img_id in results['random_q'].keys():\n",
    "            for i, answer in enumerate(results['random_q'][img_id]):\n",
    "                if answer.strip().lower() == item['random_a'][i].strip().lower():\n",
    "                    scores[\"random\"] += 1\n",
    "                        \n",
    "    QUESTIONS_PER_ITEM = len(dataset['validation'][0]['adversarial_q'])\n",
    "\n",
    "    main_result = {\n",
    "        \"adversarial\": np.round(scores[\"adversarial\"] / (len(dataset['validation']) * QUESTIONS_PER_ITEM), 4),\n",
    "        \"popular\": np.round(scores[\"popular\"] / (len(dataset['validation']) * QUESTIONS_PER_ITEM), 4),\n",
    "        \"random\": np.round(scores[\"random\"] / (len(dataset['validation']) * QUESTIONS_PER_ITEM), 4)\n",
    "    }\n",
    "    print(\"Adversarial: \", main_result[\"adversarial\"])\n",
    "    print(\"Popular: \", main_result[\"popular\"])\n",
    "    print(\"Random: \", main_result[\"random\"])\n",
    "    return main_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3c2f7b-fd0f-42f7-ab14-f1003e4e2a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial:  0.89\n",
      "Popular:  0.9033\n",
      "Random:  0.912\n",
      "Adversarial:  0.803\n",
      "Popular:  0.8557\n",
      "Random:  0.9073\n",
      "Adversarial:  0.8753\n",
      "Popular:  0.8893\n",
      "Random:  0.8983\n",
      "Adversarial:  0.801\n",
      "Popular:  0.8463\n",
      "Random:  0.887\n",
      "Adversarial:  0.8647\n",
      "Popular:  0.868\n",
      "Random:  0.8753\n",
      "Adversarial:  0.7867\n",
      "Popular:  0.8287\n",
      "Random:  0.8747\n"
     ]
    }
   ],
   "source": [
    "RESULTS_PATH = os.path.join(ROOT,\"results/recorded_runs/\") # no need to specify absolute path if the experiments were done by you\n",
    "#RESULTS_PATH = \"/scratch/cs/world-models/dain/open-world-symbolic-planner/results/benchmark/\"\n",
    "\n",
    "wino_results_files = {\n",
    "    \"Llava OneVision\":os.path.join(RESULTS_PATH, \"Winoground\", \"results_5395859.json\"),\n",
    "    \"Qwen2-VL\":os.path.join(RESULTS_PATH, \"Winoground\", \"results_5395860.json\"),\n",
    "    \"Qwen2.5-VL\":os.path.join(RESULTS_PATH, \"Winoground\", \"results_5494884.json\")\n",
    "}\n",
    "\n",
    "coco_results_files = {\n",
    "    \"Llava OneVision\":os.path.join(RESULTS_PATH, \"POPE\", \"COCO\", \"results_5395856.json\"),\n",
    "    \"Qwen2-VL\":os.path.join(RESULTS_PATH, \"POPE\", \"COCO\", \"results_5395858.json\"),\n",
    "    \"Qwen2.5-VL\":os.path.join(RESULTS_PATH, \"POPE\", \"COCO\", \"results_5492567.json\")\n",
    "}\n",
    "\n",
    "oi_results_files = {\n",
    "    \"Llava OneVision\":os.path.join(RESULTS_PATH, \"POPE\", \"OpenImages\", \"results_5396798.json\"),\n",
    "    \"Qwen2-VL\":os.path.join(RESULTS_PATH, \"POPE\", \"OpenImages\", \"results_5395857.json\"),\n",
    "    \"Qwen2.5-VL\":os.path.join(RESULTS_PATH, \"POPE\", \"OpenImages\", \"results_5493086.json\")\n",
    "}\n",
    "\n",
    "COCO_HF_PATH = \"/scratch/cs/world-models/predicate_datasets/POPE/output/coco/hf_coco_pope_dataset\"\n",
    "coco_hf_dataset = datasets.load_from_disk(COCO_HF_PATH)\n",
    "\n",
    "OI_HF_PATH = \"/scratch/cs/world-models/predicate_datasets/POPE/output/openimages/hf_openimages_pope_dataset_500\"\n",
    "oi_hf_dataset = datasets.load_from_disk(OI_HF_PATH)\n",
    "\n",
    "main_results = defaultdict(dict)\n",
    "for model in wino_results_files:\n",
    "\n",
    "    # Winoground\n",
    "    wino_results = json.load(open(wino_results_files[model]))\n",
    "    wino_scores = compute_winoground_main_results(wino_results)\n",
    "    main_results[model]['winoground'] = {'overall':wino_scores['overall_frac']}\n",
    "\n",
    "    # COCO pope\n",
    "    coco_results = json.load(open(coco_results_files[model]))\n",
    "    coco_scores = compute_coco_main_results(coco_results, coco_hf_dataset)\n",
    "    main_results[model]['coco_pope'] = {\n",
    "        'adversarial':coco_scores['adversarial'],\n",
    "        'popular':coco_scores['popular'],\n",
    "        'random':coco_scores['random'],\n",
    "    }\n",
    "    # OpenImages pope\n",
    "    oi_results = json.load(open(oi_results_files[model]))\n",
    "    oi_scores = compute_oi_main_results(oi_results, oi_hf_dataset) # is there any difference in the function with compute_coco_main_results?\n",
    "    main_results[model]['open_images_pope'] = {\n",
    "        'adversarial':oi_scores['adversarial'],\n",
    "        'popular':oi_scores['popular'],\n",
    "        'random':oi_scores['random'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faaa5ed4-98fa-452c-be47-6ec272ae9e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'Llava OneVision': {'winoground': {'overall': 0.659375},\n",
       "              'coco_pope': {'adversarial': 0.89,\n",
       "               'popular': 0.9033,\n",
       "               'random': 0.912},\n",
       "              'open_images_pope': {'adversarial': 0.803,\n",
       "               'popular': 0.8557,\n",
       "               'random': 0.9073}},\n",
       "             'Qwen2-VL': {'winoground': {'overall': 0.728125},\n",
       "              'coco_pope': {'adversarial': 0.8753,\n",
       "               'popular': 0.8893,\n",
       "               'random': 0.8983},\n",
       "              'open_images_pope': {'adversarial': 0.801,\n",
       "               'popular': 0.8463,\n",
       "               'random': 0.887}},\n",
       "             'Qwen2.5-VL': {'winoground': {'overall': 0.7375},\n",
       "              'coco_pope': {'adversarial': 0.8647,\n",
       "               'popular': 0.868,\n",
       "               'random': 0.8753},\n",
       "              'open_images_pope': {'adversarial': 0.7867,\n",
       "               'popular': 0.8287,\n",
       "               'random': 0.8747}}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b169878-ad37-4e56-b821-370097989169",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.path.dirname(os.path.dirname(os.path.abspath('.')))\n",
    "results_path = os.path.join(ROOT, 'results/official/V0/llava-qwen-qwen2.5-03-02-25')\n",
    "os.makedirs(results_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53d2ba6e-ed20-4bdb-85aa-7c60b4abfd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save main results\n",
    "main_results_path = os.path.join(results_path, \"results.json\")\n",
    "with open(main_results_path, \"w\") as f:\n",
    "    json.dump(main_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30864dba-9dca-4810-b206-2147bc99a186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  Llava OneVision\n",
      "experiments:  {'winoground': {'overall': 0.659375}, 'coco_pope': {'adversarial': 0.89, 'popular': 0.9033, 'random': 0.912}, 'open_images_pope': {'adversarial': 0.803, 'popular': 0.8557, 'random': 0.9073}}\n",
      "experiment:  winoground\n",
      "wino_results_files[model_name]:  /scratch/cs/world-models/merlerm1/open-world-symbolic-planner/results/recorded_runs/Winoground/results_5395859.json\n",
      "experiment:  coco_pope\n",
      "coco_results_files[model_name]:  /scratch/cs/world-models/merlerm1/open-world-symbolic-planner/results/recorded_runs/POPE/COCO/results_5395856.json\n",
      "experiment:  open_images_pope\n",
      "oi_results_files[model_name]:  /scratch/cs/world-models/merlerm1/open-world-symbolic-planner/results/recorded_runs/POPE/OpenImages/results_5396798.json\n",
      "model_name:  Qwen2-VL\n",
      "experiments:  {'winoground': {'overall': 0.728125}, 'coco_pope': {'adversarial': 0.8753, 'popular': 0.8893, 'random': 0.8983}, 'open_images_pope': {'adversarial': 0.801, 'popular': 0.8463, 'random': 0.887}}\n",
      "experiment:  winoground\n",
      "wino_results_files[model_name]:  /scratch/cs/world-models/merlerm1/open-world-symbolic-planner/results/recorded_runs/Winoground/results_5395860.json\n",
      "experiment:  coco_pope\n",
      "coco_results_files[model_name]:  /scratch/cs/world-models/merlerm1/open-world-symbolic-planner/results/recorded_runs/POPE/COCO/results_5395858.json\n",
      "experiment:  open_images_pope\n",
      "oi_results_files[model_name]:  /scratch/cs/world-models/merlerm1/open-world-symbolic-planner/results/recorded_runs/POPE/OpenImages/results_5395857.json\n",
      "model_name:  Qwen2.5-VL\n",
      "experiments:  {'winoground': {'overall': 0.7375}, 'coco_pope': {'adversarial': 0.8647, 'popular': 0.868, 'random': 0.8753}, 'open_images_pope': {'adversarial': 0.7867, 'popular': 0.8287, 'random': 0.8747}}\n",
      "experiment:  winoground\n",
      "wino_results_files[model_name]:  /scratch/cs/world-models/merlerm1/open-world-symbolic-planner/results/recorded_runs/Winoground/results_5494884.json\n",
      "experiment:  coco_pope\n",
      "coco_results_files[model_name]:  /scratch/cs/world-models/merlerm1/open-world-symbolic-planner/results/recorded_runs/POPE/COCO/results_5492567.json\n",
      "experiment:  open_images_pope\n",
      "oi_results_files[model_name]:  /scratch/cs/world-models/merlerm1/open-world-symbolic-planner/results/recorded_runs/POPE/OpenImages/results_5493086.json\n"
     ]
    }
   ],
   "source": [
    "# Save detailed results for POPE datasets\n",
    "\n",
    "detailed_model_results = defaultdict(dict)\n",
    "for model_name, experiments in main_results.items():\n",
    "    print(\"model_name: \", model_name)\n",
    "    print(\"experiments: \", experiments)\n",
    "    \n",
    "    for experiment in experiments:\n",
    "        print(\"experiment: \", experiment)\n",
    "        if experiment == \"winoground\":\n",
    "            print(\"wino_results_files[model_name]: \", wino_results_files[model_name])\n",
    "            wino_results = json.load(open(wino_results_files[model_name]))\n",
    "            detailed_model_results[model_name]['winoground'] = wino_results\n",
    "            \n",
    "        elif experiment in [\"coco_pope\", \"open_images_pope\"]:\n",
    "            if experiment == \"coco_pope\":\n",
    "                print(\"coco_results_files[model_name]: \", coco_results_files[model_name])\n",
    "                pope_results = json.load(open(coco_results_files[model_name]))\n",
    "            else:\n",
    "                print(\"oi_results_files[model_name]: \", oi_results_files[model_name])\n",
    "                pope_results = json.load(open(oi_results_files[model_name]))\n",
    "\n",
    "            # This part right now is very ugly, but we could fix it in benchmark_VLM.py directly\n",
    "            # from {'adversarial_q':{'id_1':['yes', 'no', 'yes', 'no', 'yes', 'no'], 'id_2': ['yes', 'no', 'yes', 'no', 'yes', 'no'], ...}, 'popular_q':{...}, 'random_q':{...}}\n",
    "            # to {['id_1':{'adversarial_a':[correct, correct, correct, correct, correct, correct], 'popular_a':[...], 'random_a':[...]}, 'id_2':{...}, ...]} -> not sure about the switch adversarial_q to adversarial_a ...\n",
    "            # where correct is True or False depending if the answer is correct or not!\n",
    "            # leave confidence placeholders to [None, None, None, None, None, None] for each answer\n",
    "            \n",
    "            detailed_model_results[model_name][experiment] = []\n",
    "\n",
    "            for j, ID in enumerate(pope_results['adversarial_q'].keys()):\n",
    "                if experiment == \"coco_pope\":\n",
    "                    val = coco_hf_dataset['validation'] \n",
    "                else:\n",
    "                    val = oi_hf_dataset['validation'] \n",
    "\n",
    "                assert str(val[j]['id'])==ID, f\"Different ordering of dataset and answers {val[j]['id']} and {ID}\"\n",
    "\n",
    "                adversarial_a = []\n",
    "                for i, answer in enumerate(pope_results['adversarial_q'][ID]):\n",
    "                    correct = True if answer.strip().lower() == val[j]['adversarial_a'][i].strip().lower() else False\n",
    "                    adversarial_a.append(correct)\n",
    "\n",
    "                popular_a = []\n",
    "                for i, answer in enumerate(pope_results['popular_q'][ID]):\n",
    "                    correct = True if answer.strip().lower() == val[j]['popular_a'][i].strip().lower() else False\n",
    "                    popular_a.append(correct)\n",
    "\n",
    "                random_a = []\n",
    "                for i, answer in enumerate(pope_results['random_q'][ID]):\n",
    "                    correct = True if answer.strip().lower() == val[j]['random_a'][i].strip().lower() else False\n",
    "                    random_a.append(correct)\n",
    "\n",
    "                \n",
    "                detailed_model_results[model_name][experiment].append({\n",
    "                    \"id\": ID,\n",
    "                    \"adversarial_a\": adversarial_a, \n",
    "                    \"popular_a\": popular_a, \n",
    "                    \"random_a\": random_a, \n",
    "                    \"adversarial_conf\": [None, None, None, None, None, None],\n",
    "                    \"popular_conf\": [None, None, None, None, None, None],\n",
    "                    \"random_conf\": [None, None, None, None, None, None]\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Experiment names supported right now are 'winoground', 'coco_pope' and 'open_images_pope', received {experiment}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "997f669b-a84d-45e3-94ec-ab999628d68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Llava OneVision', 'Qwen2-VL', 'Qwen2.5-VL'])\n",
      "dict_keys(['winoground', 'coco_pope', 'open_images_pope'])\n",
      "dict_keys(['results', 'model'])\n",
      "500\n",
      "{'id': '74', 'adversarial_a': [True, True, True, True, True, True], 'popular_a': [True, True, True, True, True, True], 'random_a': [True, True, True, True, True, True], 'adversarial_conf': [None, None, None, None, None, None], 'popular_conf': [None, None, None, None, None, None], 'random_conf': [None, None, None, None, None, None]}\n",
      "500\n",
      "{'id': '2fed663b4eb60fc8', 'adversarial_a': [True, True, True, True, True, True], 'popular_a': [True, True, True, True, True, True], 'random_a': [True, True, True, True, True, True], 'adversarial_conf': [None, None, None, None, None, None], 'popular_conf': [None, None, None, None, None, None], 'random_conf': [None, None, None, None, None, None]}\n"
     ]
    }
   ],
   "source": [
    "print(detailed_model_results.keys())\n",
    "print(detailed_model_results['Llava OneVision'].keys())\n",
    "print(detailed_model_results['Llava OneVision']['winoground'].keys())\n",
    "print(len(detailed_model_results['Llava OneVision']['coco_pope']))\n",
    "print(detailed_model_results['Llava OneVision']['coco_pope'][0])\n",
    "print(len(detailed_model_results['Llava OneVision']['open_images_pope']))\n",
    "print(detailed_model_results['Llava OneVision']['open_images_pope'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a373a81e-ea6a-4056-b96d-fbe788b76ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results for each model\n",
    "for model_name, results in detailed_model_results.items():\n",
    "    model_results_path = os.path.join(results_path, f\"{model_name}.json\")\n",
    "    with open(model_results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4135fce8-4acf-4fc8-81b2-aeae640c5c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata \n",
    "metadata = {\n",
    "    \"experiment_date\": \"31-01-2025\",\n",
    "    \"model_names\": list(main_results.keys()),\n",
    "    \"dataset_names\": list(next(iter(main_results.values()))), # ['winoground', 'coco_pope', 'open_images_pope'], don't ask me how it works\n",
    "}\n",
    "metadata_path = os.path.join(results_path, \"metadata.json\")\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328322f1-2e4b-4be1-946e-d2df43aa1176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
