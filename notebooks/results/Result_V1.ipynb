{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ea758a-701e-455a-ab6f-38ad04913aa8",
   "metadata": {},
   "source": [
    "# Results notebook\n",
    "\n",
    "**Workflow:**\n",
    "1. Specify which version of results to load\n",
    "    - Maybe it could be a folder such as `results/official/V0/test-27-01-25` with internal structure something like: `metadata.json`, `results.json`, `{model_name_1}.json`, ... ,`{model_name_n}.json`\n",
    "    - Not sure where is the best place to store everything, I think:\n",
    "        -  `results.json` for the main benchmark results (like a models x experiments table)\n",
    "        -  `{model_name_i}.json` could be used for storing the detailed Yes/No answers for each experiment and all samples, so that we can use that for in-depth analysis. Also confidence scores would be nice.\n",
    "        -  `metadata.json` to be discussed, anything from just the date of the experiments, to the complete list of hyperparameters of every experiment\n",
    "2. Load `results.json` and print them for the summary of the results; later on we might also explore radio-plots or similar visualizations for a breakdown into skills of the results\n",
    "3. Quantitative section: this can be a more in-depth analysis of the benchmark, which can also be used for deciding which images to inspect for the qualitative analysis\n",
    "4. Qualitative section: pick either a model and look at all experiments, or an experiment and look at all the models, visualize confidence scores, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb9238a-f462-41c8-90c9-c116603265d9",
   "metadata": {},
   "source": [
    "### Some nice-to-have features:\n",
    "\n",
    "- [x] Ability to manually inspect *n* images with questions where the model was wrong for every part of the benchmark  \n",
    "- [x] Visualize model confidence in the answers  \n",
    "- [ ] Show answers from different models for the same image  \n",
    "- [ ] Compare correlation of fails between different models (e.g., do they have the same weaknesses?)  \n",
    "- [x] Perform all of the above without needing to load and interact with the models\n",
    "- [ ] Print table(s) of results to LaTeX format for easy copy-paste  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56e51004-9f10-4b9c-81ff-e62d4a39997f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT /Users/pietroferrazzi/Desktop/dottorato/AAA_progetti/codice/open-world-symbolic-planner\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "ROOT = os.path.dirname(os.path.dirname(os.path.abspath('.')))\n",
    "print(\"ROOT\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21c354b-f579-4f14-a89f-33bbed6b37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234963a5-9bd0-4a1a-9193-28b948f4d7c5",
   "metadata": {},
   "source": [
    "## Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca446ee4-2049-43d2-8c73-f7bbb90d5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_folder = os.path.join(ROOT, 'results/official/V1/pope-bias-15-02-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-vg-18-02-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-oi-cleaned-24-02-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-oi-cleaned-26-02-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-blocksworld-27-02-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-blocksworldv2-28-02-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-blocksworld-3-03-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-blocksworld_precond-3-03-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-blocksworld_precond_cot-10-03-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-72-blocksworld_precond-10-03-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-blocksworld-column-labels-10-03-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-oi-cot-10-03-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-blocksworld-column-labels_shuffled-10-03-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-blocksworld-column-labels_symbolic-10-03-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-blocksworld-column-labels-v2-11-03-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-blocksworld-column-labels-shuffled-v2-11-03-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-blocksworld-column-labels-symbolic-v2-11-03-25')\n",
    "#results_folder = os.path.join(ROOT, 'results/official/V1/llava-qwen-blocksworld-detailed-prompt-12-03-25')\n",
    "results_folder = os.path.join(ROOT, 'results/official/V1/blocksword-aya-mistral-qwen-llava-deepseek-molmo-14-04-25')\n",
    "\n",
    "main_results_path = os.path.join(results_folder, 'results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9ffb20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                Dataset                   Adversarial  Popular      Random       Positive     Negative    \n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Llava OneVision      blocksworld               -            -            -            0.70         0.76        \n",
      "Qwen2-VL             blocksworld               -            -            -            0.57         0.81        \n",
      "Qwen2.5-VL           blocksworld               -            -            -            0.65         0.92        \n",
      "DeepSeek VL2 tiny    blocksworld               -            -            -            0.21         0.94        \n",
      "DeepSeek VL2         blocksworld               -            -            -            0.25         0.95        \n",
      "Mistral Small 3.1    blocksworld               -            -            -            0.65         0.84        \n",
      "Aya-vision 8B        blocksworld               -            -            -            0.57         0.65        \n",
      "Aya-vision 32B       blocksworld               -            -            -            0.90         0.67        \n",
      "Molmo                blocksworld               -            -            -            0.65         0.82        \n"
     ]
    }
   ],
   "source": [
    "# Load main results\n",
    "with open(main_results_path, \"r\") as f:\n",
    "    main_results = json.load(f)\n",
    "\n",
    "# Print as a table\n",
    "header = f\"{'Model':<20} {'Dataset':<25} {'Adversarial':<12} {'Popular':<12} {'Random':<12} {'Positive':<12} {'Negative':<12}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for model, experiments in main_results.items():\n",
    "    for exp, metrics in experiments.items():\n",
    "        if 'block' not in exp :\n",
    "            continue\n",
    "        for k in metrics:\n",
    "            if metrics[k] is None:\n",
    "                metrics[k] = np.nan\n",
    "        if ('pope' in exp) or ('bias' in exp):\n",
    "            row = f\"{model:<20} {exp:<25} {metrics['adversarial']:<12.2f} {metrics['popular']:<12.2f} {metrics['random']:<12.2f} {metrics['positive']:<12.2f} {'-':<12}\"\n",
    "        else:\n",
    "            if 'positive' not in metrics:\n",
    "                continue # blocksworld precondition effects has different metrics, print later\n",
    "            row = f\"{model:<20} {exp:<25} {'-':<12} {'-':<12} {'-':<12} {metrics['positive']:<12.2f} {metrics['negative']:<12.2f}\"\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f84f80-73e2-4eca-9db0-86790e8c6f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                Metric                                   Strict          Precond sat  Precond unsat Effect changed Effect unchanged\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "Llava OneVision      blocksworld_precondition_effect          Non-strict      0.66         0.72          0.77           0.68        \n",
      "Llava OneVision      blocksworld_precondition_effect          Strict          0.39         0.49          0.46           0.30        \n",
      "Qwen2-VL             blocksworld_precondition_effect          Non-strict      0.65         0.66          0.69           0.66        \n",
      "Qwen2-VL             blocksworld_precondition_effect          Strict          0.37         0.43          0.31           0.28        \n",
      "Qwen2.5-VL           blocksworld_precondition_effect          Non-strict      0.63         0.79          0.79           0.78        \n",
      "Qwen2.5-VL           blocksworld_precondition_effect          Strict          0.37         0.62          0.49           0.45        \n",
      "DeepSeek VL2 tiny    blocksworld_precondition_effect          Non-strict      0.53         0.67          0.64           0.65        \n",
      "DeepSeek VL2 tiny    blocksworld_precondition_effect          Strict          0.20         0.52          0.33           0.29        \n",
      "DeepSeek VL2         blocksworld_precondition_effect          Non-strict      0.50         0.65          0.66           0.70        \n",
      "DeepSeek VL2         blocksworld_precondition_effect          Strict          0.08         0.54          0.34           0.34        \n",
      "Mistral Small 3.1    blocksworld_precondition_effect          Non-strict      0.78         0.84          0.88           0.84        \n",
      "Mistral Small 3.1    blocksworld_precondition_effect          Strict          0.61         0.73          0.70           0.57        \n",
      "Aya-vision 8B        blocksworld_precondition_effect          Non-strict      0.74         0.65          0.57           0.73        \n",
      "Aya-vision 8B        blocksworld_precondition_effect          Strict          0.56         0.42          0.16           0.38        \n",
      "Aya-vision 32B       blocksworld_precondition_effect          Non-strict      0.79         0.55          0.68           0.67        \n",
      "Aya-vision 32B       blocksworld_precondition_effect          Strict          0.58         0.31          0.36           0.34        \n",
      "Qwen2.5-VL 72B       blocksworld_precondition_effect          Non-strict      0.84         0.86          0.87           0.88        \n",
      "Qwen2.5-VL 72B       blocksworld_precondition_effect          Strict          0.71         0.76          0.68           0.70        \n"
     ]
    }
   ],
   "source": [
    "header = f\"{'Model':<20} {'Metric':<40} {'Strict':<15} {'Precond sat':<12} {'Precond unsat':<13} {'Effect changed':<14} {'Effect unchanged':<12}\"\n",
    "\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for model, experiments in main_results.items():\n",
    "    for exp, metrics in experiments.items():\n",
    "        if not ('precondition_effect' in exp):\n",
    "            continue\n",
    "        for k in metrics:\n",
    "            if metrics[k] is None:\n",
    "                metrics[k] = np.nan\n",
    "        row = f\"{model:<20} {exp:<40} {'Non-strict':<15} {metrics['precond_satisfied']:<12.2f} {metrics['precond_unsatisfied']:<13.2f} {metrics['effect_changed']:<14.2f} {metrics['effect_unchanged']:<12.2f}\"\n",
    "        print(row)\n",
    "        row_strict = f\"{model:<20} {exp:<40} {'Strict':<15} {metrics['precond_satisfied_strict']:<12.2f} {metrics['precond_unsatisfied_strict']:<13.2f} {metrics['effect_changed_strict']:<14.2f} {metrics['effect_unchanged_strict']:<12.2f}\"\n",
    "        print(row_strict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57abb66-1a6b-4e9a-b4d2-8121cbf2eda6",
   "metadata": {},
   "source": [
    "## Quantitative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a649cf",
   "metadata": {},
   "source": [
    "### Blocksworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c186e513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /opt/homebrew/anaconda3/envs/nlp/lib/python3.11/site-packages (0.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54cfa4-39ec-4184-8e55-42fe3ceaa6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_predicate = {model: defaultdict(dict) for model in main_results.keys()}\n",
    "for model, experiments in main_results.items():\n",
    "    if 'blocksworld' not in experiments:\n",
    "        continue\n",
    "    detailed_results_path = os.path.join(results_folder, f'{model}.json')\n",
    "    with open(detailed_results_path, \"r\") as f:\n",
    "        detailed_results = json.load(f)\n",
    "    for question_id, data in detailed_results['blocksworld'].items():\n",
    "        predicate = data['metadata']['predicate']\n",
    "        if predicate not in results_per_predicate[model]:\n",
    "            results_per_predicate[model][predicate] = {'positive': [], 'negative': []}\n",
    "        results_per_predicate[model][predicate][data['split']].append(data['correct'])\n",
    "\n",
    "dfs = []\n",
    "for model, predicates in results_per_predicate.items():\n",
    "    for predicate, splits in predicates.items():\n",
    "        for split, correct in splits.items():\n",
    "            if len(correct) == 0:\n",
    "                continue\n",
    "            acc = np.mean(correct)\n",
    "            dfs.append(pd.DataFrame({'Model': model, 'Predicate': predicate, 'Split': split, 'Accuracy': acc}, index=[0]))\n",
    "df = pd.concat(dfs)\n",
    "df_positive = df[df['Split'] == 'positive']\n",
    "df_negative = df[df['Split'] == 'negative']\n",
    "df_positive = df_positive.pivot(index='Model', columns='Predicate', values='Accuracy')\n",
    "df_negative = df_negative.pivot(index='Model', columns='Predicate', values='Accuracy')\n",
    "\n",
    "print('Positive')\n",
    "print(df_positive.to_markdown())\n",
    "print()\n",
    "print('Negative')\n",
    "print(df_negative.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present the same data in two barplots one for positive and one for negative with the predicate name on the x-axis and the accuracy on the y-axis, colored by model.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_positive = df_positive.reset_index()\n",
    "df_negative = df_negative.reset_index()\n",
    "\n",
    "df_positive = df_positive.melt(id_vars='Model', var_name='Predicate', value_name='Accuracy')\n",
    "df_negative = df_negative.melt(id_vars='Model', var_name='Predicate', value_name='Accuracy')\n",
    "\n",
    "fig, ax = matplotlib.pyplot.subplots(1, 2, figsize=(20, 5))\n",
    "sns.barplot(data=df_positive, x='Predicate', y='Accuracy', hue='Model', ax=ax[0])\n",
    "ax[0].set_title('Positive')\n",
    "ax[0].set_ylim(0, 1)\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_xlabel('Predicate')\n",
    "ax[0].legend(title='Model', loc='upper left')\n",
    "\n",
    "sns.barplot(data=df_negative, x='Predicate', y='Accuracy', hue='Model', ax=ax[1])\n",
    "ax[1].set_title('Negative')\n",
    "ax[1].set_ylim(0, 1)\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_xlabel('Predicate')\n",
    "ax[1].legend(title='Model', loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e44888",
   "metadata": {},
   "source": [
    "### Blocksworld preconditions effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ae60925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicate\n",
      "clear       0.673822\n",
      "incolumn    0.746970\n",
      "on          0.907865\n",
      "Name: Accuracy, dtype: float64\n",
      "Precondition satisfied\n",
      "| Model           |    clear |   incolumn |\n",
      "|:----------------|---------:|-----------:|\n",
      "| Llava OneVision | 0.656566 |   0.656566 |\n",
      "| Molmo           | 0.666667 |   0.676768 |\n",
      "| Qwen2-VL        | 0.585859 |   0.707071 |\n",
      "| Qwen2.5-VL      | 0.59596  |   0.656566 |\n",
      "| Qwen2.5-VL 72B  | 0.909091 |   0.777778 |\n",
      "\n",
      "Precondition unsatisfied\n",
      "| Model           |    clear |   incolumn |\n",
      "|:----------------|---------:|-----------:|\n",
      "| Llava OneVision | 0.616162 |   0.828283 |\n",
      "| Molmo           | 0.707071 |   0.777778 |\n",
      "| Qwen2-VL        | 0.474747 |   0.848485 |\n",
      "| Qwen2.5-VL      | 0.808081 |   0.777778 |\n",
      "| Qwen2.5-VL 72B  | 0.939394 |   0.787879 |\n",
      "\n",
      "Effect changed\n",
      "| Model           |    clear |   incolumn |       on |\n",
      "|:----------------|---------:|-----------:|---------:|\n",
      "| Llava OneVision | 0.606742 |   0.777778 | 0.910112 |\n",
      "| Molmo           | 0.662921 |   0.747475 | 0.876404 |\n",
      "| Qwen2-VL        | 0.382022 |   0.777778 | 0.898876 |\n",
      "| Qwen2.5-VL      | 0.707865 |   0.686869 | 0.988764 |\n",
      "| Qwen2.5-VL 72B  | 0.94382  |   0.727273 | 0.955056 |\n",
      "\n",
      "Effect unchanged\n",
      "| Model           |    clear |   incolumn |       on |\n",
      "|:----------------|---------:|-----------:|---------:|\n",
      "| Llava OneVision | 0.561798 |   0.686869 | 0.786517 |\n",
      "| Molmo           | 0.685393 |   0.757576 | 0.932584 |\n",
      "| Qwen2-VL        | 0.438202 |   0.747475 | 0.797753 |\n",
      "| Qwen2.5-VL      | 0.595506 |   0.757576 | 0.977528 |\n",
      "| Qwen2.5-VL 72B  | 0.932584 |   0.777778 | 0.955056 |\n"
     ]
    }
   ],
   "source": [
    "results_per_predicate = {model: defaultdict(dict) for model in main_results.keys()}\n",
    "for model, experiments in main_results.items():\n",
    "    if 'blocksworld_precondition_effect' not in experiments:\n",
    "        continue\n",
    "    detailed_results_path = os.path.join(results_folder, f'{model}.json')\n",
    "    with open(detailed_results_path, \"r\") as f:\n",
    "        detailed_results = json.load(f)\n",
    "    for question_id, data in detailed_results['blocksworld_precondition_effect'].items():\n",
    "        predicate = data['metadata']['predicate']\n",
    "        if predicate not in results_per_predicate[model]:\n",
    "            results_per_predicate[model][predicate] = {'precond_satisfied': [], 'precond_unsatisfied': [], 'effect_changed': [], 'effect_unchanged': []}\n",
    "        results_per_predicate[model][predicate][data['split']].append(data['correct'])\n",
    "\n",
    "dfs = []\n",
    "for model, predicates in results_per_predicate.items():\n",
    "    for predicate, splits in predicates.items():\n",
    "        for split, correct in splits.items():\n",
    "            if len(correct) == 0:\n",
    "                continue\n",
    "            acc = np.mean(correct)\n",
    "            dfs.append(pd.DataFrame({'Model': model, 'Predicate': predicate, 'Split': split, 'Accuracy': acc}, index=[0]))\n",
    "df = pd.concat(dfs)\n",
    "df_precond_satisfied = df[df['Split'] == 'precond_satisfied']\n",
    "df_precond_unsatisfied = df[df['Split'] == 'precond_unsatisfied']\n",
    "df_effect_changed = df[df['Split'] == 'effect_changed']\n",
    "df_effect_unchanged = df[df['Split'] == 'effect_unchanged']\n",
    "\n",
    "df_precond_satisfied = df_precond_satisfied.pivot(index='Model', columns='Predicate', values='Accuracy')\n",
    "df_precond_unsatisfied = df_precond_unsatisfied.pivot(index='Model', columns='Predicate', values='Accuracy')\n",
    "df_effect_changed = df_effect_changed.pivot(index='Model', columns='Predicate', values='Accuracy')\n",
    "df_effect_unchanged = df_effect_unchanged.pivot(index='Model', columns='Predicate', values='Accuracy')\n",
    "\n",
    "# Also add a row that prints the average accuracy for each predicate on df (independent of the split)\n",
    "# For each predicate in df print the average accuracy (independent of the split and the model)\n",
    "print(df.groupby('Predicate')['Accuracy'].mean())\n",
    "\n",
    "print('Precondition satisfied')\n",
    "print(df_precond_satisfied.to_markdown())\n",
    "print()\n",
    "\n",
    "print('Precondition unsatisfied')\n",
    "print(df_precond_unsatisfied.to_markdown())\n",
    "print()\n",
    "\n",
    "print('Effect changed')\n",
    "print(df_effect_changed.to_markdown())\n",
    "print()\n",
    "\n",
    "print('Effect unchanged')\n",
    "print(df_effect_unchanged.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b8a0e2-9f71-4b15-aeae-05799365c95d",
   "metadata": {},
   "source": [
    "## Qualitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b71e748-0d90-4c19-907d-6367a1c90d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_examples(results, data_path, samples_per_split=5):\n",
    "\n",
    "    all_splits = np.unique([x['split'] for x in results.values()]).tolist()\n",
    "    failures_per_split = {split:0 for split in all_splits}\n",
    "    \n",
    "    for res in results.values():\n",
    "        split = res['split']\n",
    "        \n",
    "        if failures_per_split[split] < samples_per_split:\n",
    "            failures_per_split[split] += 1\n",
    "            print(f\"\\n\\nSplit: {split}\")\n",
    "            print(f\"Q:{res['question']} - GT answer: {res['gt_answer']} - Model answer: {res['answer']}\")\n",
    "            image = Image.open(os.path.join(data_path, res['image_name']))\n",
    "            image.show()\n",
    "            \n",
    "        if np.all([count >= samples_per_split for count in failures_per_split.values()]):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ac3491c-c085-460e-9400-63c6bcce5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_failures(results, data_path, samples_per_split=5):\n",
    "\n",
    "    all_splits = np.unique([x['split'] for x in results.values()]).tolist()\n",
    "    failures_per_split = {split:0 for split in all_splits}\n",
    "    \n",
    "    for res in results.values():\n",
    "        split = res['split']\n",
    "        \n",
    "        if failures_per_split[split] < samples_per_split and not res['correct']:\n",
    "            failures_per_split[split] += 1\n",
    "            print(f\"\\n\\nSplit: {split} ({failures_per_split[split]}/{samples_per_split}) - ImageID: {res['image_id']}\")\n",
    "            print(f\"Q:{res['question']} - GT answer: {res['gt_answer']} - Model answer: {res['answer']}\")\n",
    "            image = Image.open(os.path.join(data_path, res['image_name']))\n",
    "            image.show()\n",
    "            \n",
    "        if np.all([count >= samples_per_split for count in failures_per_split.values()]):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f98de5-01dc-4c8d-84c3-6d17e2cc2a86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e53db-6369-4269-a22b-a4a43c47da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and display n images based on a list of indices of the dataset\n",
    "#COCO_HF_PATH = \"/scratch/cs/world-models/predicate_datasets/POPE/output/coco/hf_coco_pope_dataset\" # not needed\n",
    "COCO_DATA_PATH = \"/scratch/cs/world-models/predicate_datasets/coco/val2014\"\n",
    "#dataset = datasets.load_from_disk(COCO_HF_PATH)['validation'] # not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e649c-3b0e-4a8c-a9b3-07a575044b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_inspect = 'Llava OneVision'\n",
    "model_results_path = os.path.join(results_folder, model_to_inspect+'.json')\n",
    "# Load specific results\n",
    "with open(model_results_path, \"r\") as f:\n",
    "    model_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce8dd8-aae4-44b7-8220-915266270fdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inspect_examples(model_results['coco_pope'], COCO_DATA_PATH, samples_per_split=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e240e-1d76-452b-9adf-b11591bd5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_failures(model_results['coco_pope'], COCO_DATA_PATH, samples_per_split=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4593e74-c45f-4eaf-aa3c-edd64091c373",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### OpenImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5cdba0-fb39-4fd0-bbbc-6f88667abd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and display n images based on a list of indices of the dataset\n",
    "#OI_HF_PATH = \"/scratch/cs/world-models/predicate_datasets/POPE/output/openimages/hf_openimages_pope_dataset_500\"\n",
    "OI_DATA_PATH = \"/scratch/cs/world-models/predicate_datasets/openimages/validation\"\n",
    "#dataset = datasets.load_from_disk(OI_HF_PATH)['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee9e3c-dca6-44a8-bf0e-72bd01f0bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_inspect = 'Llava OneVision'\n",
    "model_results_path = os.path.join(results_folder, model_to_inspect+'.json')\n",
    "# Load specific results\n",
    "with open(model_results_path, \"r\") as f:\n",
    "    model_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ba9f0-8dc2-49ae-880c-276dbb77a838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inspect_examples(model_results['open_images_pope'], OI_DATA_PATH, samples_per_split=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a8106-b410-4b23-99c4-eb5266d45f28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inspect_failures(model_results['open_images_pope'], OI_DATA_PATH, samples_per_split=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc02af-d201-41dc-b775-c615ccc5f7be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Winoground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d0f906-d775-48c6-9d17-7cfd6de10afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WINO_HF_PATH=\"/scratch/cs/world-models/predicate_datasets/preprocessed/winoground\"\n",
    "WINO_DATA_PATH=\"/scratch/cs/world-models/predicate_datasets/winoground_images\"\n",
    "#dataset = datasets.load_from_disk(WINO_HF_PATH)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88011c23-a29f-4f3d-bb10-abd173be7c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_inspect = 'Llava OneVision'\n",
    "model_results_path = os.path.join(results_folder, model_to_inspect+'.json')\n",
    "# Load specific results\n",
    "with open(model_results_path, \"r\") as f:\n",
    "    model_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49089b44-fe97-4ef7-a2c5-806c7a4e0a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inspect_examples(model_results['winoground'], WINO_DATA_PATH, samples_per_split=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da88e9b7-09a1-496c-ba44-b0a0c0beb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_failures(model_results['winoground'], WINO_DATA_PATH, samples_per_split=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e401f-be7f-48f1-8a87-8b13af7ba2b7",
   "metadata": {},
   "source": [
    "### OpenImages predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c558d7-a19c-4719-8f4c-80655da5dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and display n images based on a list of indices of the dataset\n",
    "#OI_HF_PATH = \"/scratch/cs/world-models/predicate_datasets/POPE/output/openimages/hf_openimages_pope_dataset_500\"\n",
    "OI_DATA_PATH = \"/scratch/cs/world-models/predicate_datasets/openimages/validation\"\n",
    "#dataset = datasets.load_from_disk(OI_HF_PATH)['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b0788-971c-48db-a236-5d4fb08ef57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_inspect = 'Llava OneVision' #'Qwen2-VL'\n",
    "model_results_path = os.path.join(results_folder, model_to_inspect+'.json')\n",
    "# Load specific results\n",
    "with open(model_results_path, \"r\") as f:\n",
    "    model_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a111c1-0451-4d10-b358-d952d9be82a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inspect_examples(model_results['oi_predicate_questions'], OI_DATA_PATH, samples_per_split=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c41d8-2a78-4ed4-a7bb-a179b14e1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_failures(model_results['oi_predicate_questions'], OI_DATA_PATH, samples_per_split=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234d9ae-6693-4f2d-89f3-4604d859d290",
   "metadata": {},
   "source": [
    "### VisualGenome predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ce8896-0cb1-4120-add5-8f98e4a50d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and display n images based on a list of indices of the dataset\n",
    "VG_DATA_PATH = \"/scratch/cs/world-models/predicate_datasets/visual_genome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b88040-9181-490e-8a0c-d1658a09f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_inspect = 'Llava OneVision'\n",
    "model_results_path = os.path.join(results_folder, model_to_inspect+'.json')\n",
    "# Load specific results\n",
    "with open(model_results_path, \"r\") as f:\n",
    "    model_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd62d1-f3f2-4368-b654-ce99578589e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_examples(model_results['visual_genome'], VG_DATA_PATH, samples_per_split=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8514be-2442-4931-9845-1bb5a1ae5f82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inspect_failures(model_results['visual_genome'], VG_DATA_PATH, samples_per_split=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c9f2e3-e4a0-44cd-9b83-4f9e4132c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "365955a1",
   "metadata": {},
   "source": [
    "### Blocksworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b87a26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCKSWORLD_DATA_PATH = \"/scratch/cs/world-models/predicate_datasets/blocksworld_predicates_v1/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4e25067",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_inspect = 'Molmo'\n",
    "model_results_path = os.path.join(results_folder, model_to_inspect+'.json')\n",
    "# Load specific results\n",
    "with open(model_results_path, \"r\") as f:\n",
    "    model_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32f6b30e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'blocksworld'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inspect_failures(\u001b[43mmodel_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblocksworld\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, BLOCKSWORLD_DATA_PATH, samples_per_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'blocksworld'"
     ]
    }
   ],
   "source": [
    "inspect_failures(model_results['blocksworld'], BLOCKSWORLD_DATA_PATH, samples_per_split=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
